{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW4 LSH \n",
    "(執行約20分鐘)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial and setting config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "sc.stop()\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"LSH\")\n",
    "conf = SparkConf().set(\"spark.default.parallelism\", 1)\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing and Constructing Shingles \n",
    "1. Using sc.wholeTextFiles() to get 101 datas first\n",
    "2. Uisng map function parsing(x) to parsing the lines to the Words of each data RDD\n",
    "3. After parsing lines, we will using map function shingle_3(x) to output the 3-shingles of each Set of Words in data\n",
    "4. After step 3, flatMap first with calling distinct() to get all possible shingles, and then label them with index(i.e. row th) by calling zipWithIndex()\n",
    "\n",
    "## parsing(x):\n",
    "1. It's a map function for parsing the words in documents by splitting:\n",
    "#####  space' '   \n",
    "##### period'.' \n",
    "##### end_line'\\n' \n",
    "##### punctuation ','\n",
    "##### 雙引號'\"'\n",
    "##### \n",
    "2. Note that \"\".join means transfer the list to string, since we will use the string attribution 'replace()'\n",
    "3. Also, I use replace instead of split since I splitting not only by one 標點符號, so I first repalce them to ''\n",
    "\n",
    "## shingle_3(x):\n",
    "1. It's a map function for genarating all possible 3-shingle by scaning every 連續的 3 words iteratively\n",
    "2. The iteration range should be len(line_list)-2, since we should finish at last 3th word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get input data => RDD\n",
    "def parsing(x):\n",
    "    while(x.find('.') != -1):\n",
    "        a = x.find('.')\n",
    "        l = list(x)\n",
    "        l[a] = ''\n",
    "        x = \"\".join(l)\n",
    "    x = x.replace(',','')\n",
    "    x = x.replace('\"','')\n",
    "    x = x.replace('\\n',' ')\n",
    "    return x\n",
    "def shingle_3(x):\n",
    "    line = x.split(' ')\n",
    "    line_list = list(line)\n",
    "    while('' in line_list):\n",
    "        line_list.remove('')\n",
    "    result = []\n",
    "    for i in range(len(line_list)-2):\n",
    "        s = line_list[i]+' '+line_list[i+1]+' '+line_list[i+2]\n",
    "        result.append(s)\n",
    "    return result\n",
    "\n",
    "shingle_table = sc.wholeTextFiles(\"athletics/\").map(lambda x:x[1]).map(parsing).filter(lambda x:x!='').map(shingle_3).flatMap(lambda x:x).distinct()\n",
    "label_shingle_table = shingle_table.map(lambda x:(\"data\",x)).zipWithIndex().map(lambda x:(x[0][0],(x[0][1],x[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Shingles/Documents Table\n",
    "1. To determine whether the shingle is in a data, we can iteratively scan every data.\n",
    "2. In each iteration, we first use sc.textFile() to open a data to RDD, and calling map function parsing2(x) to find its all 3-shingle, after that we will get a RDD with several <key,value> the key is the number of document the value is several shingles, so in order to make them to a RDD with only one data, we merge them by calling reduce function reduce_add_with_space(x,y). \n",
    "3. Join the RDD from step2 with the label_shingle_table as shingle_reference which we have generate before.\n",
    "4. Calling map function detect(x) to generate a col of Shingles/Documents Table.\n",
    "5. Once we generate a new col, we should union it with previous cols before we enter next iteration.\n",
    "6. After all iterations, we will get a RDD which <key,values> is \n",
    "#### <number_of_document(i.e. col), vector of that col>.\n",
    "\n",
    "## parsing2(x)\n",
    "1. it do the same thing as parsing(x), but we need to label the key \"data\" as join-key with label_shingle_table.\n",
    "\n",
    "## detect(x)\n",
    "1. It's a map function which do the same first as shingle(x), to generate possible 3-shingle for each data.\n",
    "2. But simultaneously, we should detect the shingle from data is same as each shingle in shingle_table, if true set 1 to that (shingle,document) index, else set 0.\n",
    "3. after this fuction we can generate a columne of Shingles/Documents Table.\n",
    "\n",
    "## reduce_add_with_space(x,y)\n",
    "1. It's the reduce function to let us merge the values(contain with several shingles) together and split each shingle with space '_' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num(num):\n",
    "    if num<10:return\"00\"+str(num)\n",
    "    elif 10<=num<100: return \"0\"+str(num)\n",
    "    else: return str(num)\n",
    "def parsing2(x):\n",
    "    x = x.replace('.','')\n",
    "    x = x.replace(',','')\n",
    "    x = x.replace('\\n',' ')\n",
    "    x = x.replace('\"','')\n",
    "    x = x.replace('  ',' ')\n",
    "    return \"data\",x\n",
    "def reduce_add_with_space(x,y):\n",
    "    return x+' '+y\n",
    "\n",
    "def detect(x):\n",
    "    line = x[1][1].split(' ')\n",
    "    line_list = list(line)\n",
    "    for i in range(len(line_list)-2):\n",
    "        s = line_list[i]+' '+line_list[i+1]+' '+line_list[i+2]\n",
    "        if s == x[1][0][0]:\n",
    "            return \"data\",(x[1][0][1],1)\n",
    "    return \"data\",(x[1][0][1],0)\n",
    "def split_data(x):\n",
    "    result = []\n",
    "    for i in x[0]:\n",
    "        result.append(i)\n",
    "    result.append(x[1])\n",
    "    return result\n",
    "cols = None\n",
    "cols_2 = None\n",
    "next_cols = None\n",
    "\n",
    "tStart = time.time()\n",
    "for i in range(101):\n",
    "    old_cols = cols\n",
    "    #print(\"iteration\"+str(i))\n",
    "    doc_num = get_num(i+1)\n",
    "    data = sc.textFile(\"athletics/\"+doc_num+\".txt\").map(parsing2).filter(lambda x:x[1]!='').reduceByKey(reduce_add_with_space)\n",
    "    join_set = label_shingle_table.join(data)\n",
    "    next_col = join_set.map(lambda x:detect(x)).groupByKey()\n",
    "    if i==0: cols = next_col\n",
    "    else:\n",
    "        cols = cols.union(next_col)\n",
    "cols = cols.zipWithIndex().map(lambda x:(x[1]+1,list(x[0][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Min-Hashing\n",
    "1. For each col in cols RDD, we calculate its hash values and find the min-hash value of it by calling a map function hashing(x)\n",
    "2. After, hashing(x), we gain the signature matrix which store at RDD for line per columne\n",
    "\n",
    "## hashing(x)\n",
    "1. Before conducting this function,I make 100 different hash function first\n",
    "2. the hash functions follow the formula (a*x+b)%p, where 'a' is a 100-size-list value random range form 1 to 101 and corresponding 100-size-list 'b' is random value range from 0 to 100, and then p is a prime number which is larger than numbers of all shingles, in 101 datas we find 26679 differents shingles, so we can set prime number p as 26681)\n",
    "3. after pre-setting, we can iteratively travel the col vector in RDD(line per col), getting hash-value for different 100 hash function and find their min_hash value for each of them(hash_functions) in each col(i.e. document) \n",
    "4. Note that in the function, the first layer of iteration (i.e. for i in range(100):) means 100 differents hash function we have make, the second layer is to scanning the col in RDD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct hash_function\n",
    "import random\n",
    "a = random.sample(range(1, 101), 100)\n",
    "b = random.sample(range(0, 100), 100)\n",
    "p = 26681\n",
    "\n",
    "import math\n",
    "def hashing(x):\n",
    "    result = []\n",
    "    col = x[0]\n",
    "    for i in range(100):\n",
    "        min_ = math.inf\n",
    "        for j in x[1]:\n",
    "            ans = math.inf\n",
    "            if j[1]==1: \n",
    "                ans = (a[i]*(int(j[0])+1)+b[i])%p\n",
    "            else: ans = math.inf\n",
    "            if min_>ans:\n",
    "                min_ = ans\n",
    "                j_t = j\n",
    "                ii = i\n",
    "            elif (min_==ans) and (ans!=math.inf):return \"Wrong\",ans,j,p,j_t,i,ii,a[i],b[i] #for debug\n",
    "        pair = ((\"h\"+str(i+1)),min_)\n",
    "        #if min_==0: pair = ((\"h\"+str(i+1)),min_,a[i],b[i],j_t)\n",
    "        result.append(pair)\n",
    "    return x[0],result\n",
    "#cols = cols.cache()\n",
    "hashed_cols = cols.map(hashing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locality-Sensitive Hashing - pre_setting\n",
    "1. In homework, we set band b = 50 and band per rows r = 2 \n",
    "2. So we should first label the col for every 2 row with a band number by using a map function label_band(x)\n",
    "3. After band_labeling, we should re_construct the document-key RDD to the new RDD with band_num as key\n",
    "4. To acheive it we can using flatMap() first to seperate every single element and then group them with same band number.\n",
    "\n",
    "### label_band(x)\n",
    "1. It's is a map function which let us mark every 2 elements in col vector with a same band number by using itration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_band(x):\n",
    "    result = []\n",
    "    for i in range(50):\n",
    "        band = i+1\n",
    "        for j in range(2):\n",
    "            pair = (str(band),(\"data\"+str(x[0]),x[1][i*2+j]))\n",
    "            result.append(pair)\n",
    "    return result\n",
    "label_band_num = hashed_cols.map(label_band)\n",
    "final_label_band_num = label_band_num.flatMap(lambda x:x).groupByKey().map(lambda x:(x[0],list(x[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locality-Sensitive Hashing - hash_to_buckets\n",
    "1. Calling a map function hash_to_buckets(x) to hash the value to a buckets by the formula x%k, where k is buckets number(we set 1000)\n",
    "2. After hashing, analysising the hashed elements inside which buckets in each Band, by calling map function get_hashing_result(x)\n",
    "\n",
    "### hash_to_buckets(x)\n",
    "1. It's a map function to calculate the hashing value (i.e. hash to a bucket) \n",
    "\n",
    "### get_hashing_result(x)\n",
    "1. It's a map function which calculate the cumulation inside each bucket in each band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hash to k buckets\n",
    "k = 1000\n",
    "def hash_to_buckets(x):\n",
    "    list_ = []\n",
    "    for i in x[1]:\n",
    "        data_name = i[0]\n",
    "        hash_func = i[1][0]\n",
    "        bucket = i[1][1]%k\n",
    "        pair = (bucket,(data_name,hash_func,i[1][1]))\n",
    "        list_.append(pair)\n",
    "    return x[0],list_\n",
    "def get_hashing_result(x):\n",
    "    buckets = [None]*k\n",
    "    for i in range(k):\n",
    "        list_ = []\n",
    "        list_.append(\"B\"+str(i))\n",
    "        buckets[i] = list_\n",
    "    for i in x[1]:\n",
    "        j = i[0]\n",
    "        try: \n",
    "            buckets[j].index(i[1][0])\n",
    "        except:\n",
    "            buckets[j].append(i[1][0])\n",
    "    \n",
    "    return x[0],buckets\n",
    "    \n",
    "after_hash_to_buckets = final_label_band_num.map(hash_to_buckets)\n",
    "after_hash_to_buckets_final = after_hash_to_buckets.map(get_hashing_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locality-Sensitive Hashing - find_candidates\n",
    "1. After hashing to bucket and get each bucket information in each band, we now want to find all the candidate pairs \n",
    "2. To find them, we can first find every data combinations in each bucket of each band by calling find_candicate_pairs(x)\n",
    "3. After that, we will get the RDD which <key,values> is pairs of datas, and since different band may give us same candidates pairs, so we can use flatMap() plus distinct() to find unique candidate pairs.\n",
    "\n",
    "### find_candicate_pairs(x):\n",
    "1. It's a map function which use iteration to find out every pair in each bucke if that bucket contain two or more data.\n",
    "2. return the all candicate pairs in the band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def find_candicate_pairs(x):\n",
    "    candicate_pairs = []\n",
    "    for i in x[1]:\n",
    "        j = i[1:]\n",
    "        size = len(j)\n",
    "        if size>1:\n",
    "            pairs = list(itertools.combinations(j, 2))\n",
    "            candicate_pairs.append(pairs)\n",
    "    return candicate_pairs\n",
    "candicate_pairs = after_hash_to_buckets_final.map(find_candicate_pairs).flatMap(lambda x:x).flatMap(lambda x:x).map(lambda x:(int(re.sub(\"data\",'',x[0])),int(re.sub(\"data\",'',x[1])))).distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locality-Sensitive Hashing - calculate_similarity\n",
    "1. After getting all unique candidate pairs, we need to calculate their Jaccard_similarity\n",
    "2. So first of all, we should give each pair their documents information(i.e. corresponding col vector) by label each pair with number of pair first and then use flatMap()to seperate them, and finally using join to append each of their required documnents information, and finally, since we have label their pair number beforehand, so we can easily group back them to pairs by calling reduce function cal_similarity(x,y)\n",
    "3. Also when calling cal_similarity(x,y), we can simultaneously calculate their jaccard_similarity, the formula is:\n",
    "### |x inetraction y|/|x union y|\n",
    "#### \n",
    "4. After getting all the similarities of all candidate pairs, we sort the RDD decreasingly.\n",
    "\n",
    "### label_pair(x)\n",
    "1. It's a map function to make me easy to debug by the pair name\n",
    "\n",
    "### delete_the_row_th(x):\n",
    "1. It's a map function which let us delete the row_name of the elements inside the col vector RDD means:\n",
    "#### (document_number,  [(0,1), (1,0), (2,1), (3,0), (4,1)....])  => (document_number, [1, 0, 1, 0, 1, ...])\n",
    "\n",
    "### change_key(x)\n",
    "1. It's a map function which help us change the key from document_number to pair_number after we have finishedd the join\n",
    "2. So that we can group back the original pair and then calculate similarity by calling cal_similarity\n",
    "\n",
    "### cal_similarity(x,y)\n",
    "1. It's a reduce function which help us find original pair first and then calculate the pair's jaccard_similarity\n",
    "2. using iteration to scan same index of two documents (cols vector) value and calculate the |x inetraction y| and |x union y|\n",
    "3. |x inetraction y|+= 1 if both two documents col_vector[index] value equal to 1 (means both of them have same shingle)\n",
    "4. |x union y| += 1 if either of them col_vector[index] equal to 1 (means either of them have a shingle which exist in shingles table)\n",
    "5. after result of step 3 and 4, we can easily calculate the similarity = |x inetraction y| and |x union y|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_pair(x):\n",
    "    p1 = (x[0][0],\"pair\"+str(x[1]))\n",
    "    p2 = (x[0][1],\"pair\"+str(x[1]))\n",
    "    return p1,p2\n",
    "def delete_the_row_th(x):\n",
    "    data = x[0]\n",
    "    result = []\n",
    "    for i in x[1]:\n",
    "        result.append(i[1])\n",
    "    return data,result\n",
    "def change_key(x):\n",
    "    key = x[1][0]\n",
    "    data = x[0]\n",
    "    return key,(x[0],x[1][1])\n",
    "def cal_similarity(x,y):\n",
    "    p1 = x[0]\n",
    "    p2 = y[0]\n",
    "    union = 0\n",
    "    inter = 0\n",
    "    for i in range(len(x[1])):\n",
    "        relation = x[1][i]+y[1][i] \n",
    "        if relation==1: #union\n",
    "            union+=1\n",
    "        elif relation==2: #inter\n",
    "            union+=1\n",
    "            inter+=1\n",
    "    similarity_jaccacrd = inter/union\n",
    "    pair = None\n",
    "    if p1<p2: pair = (p1,p2)\n",
    "    elif p1>p2: pair = (p2,p1)\n",
    "    return pair,similarity_jaccacrd\n",
    "\n",
    "cols_new = cols.map(delete_the_row_th)\n",
    "\n",
    "candicate_pairs_with_label = candicate_pairs.zipWithIndex().map(label_pair).flatMap(lambda x:x)\n",
    "##join pair with its data 1s\n",
    "candicate_with_data = candicate_pairs_with_label.join(cols_new)\n",
    "#calculate sim\n",
    "candicate_with_data_new = candicate_with_data.map(change_key)\n",
    "Final = candicate_with_data_new.reduceByKey(cal_similarity).map(lambda x:x[1]).sortBy(lambda x:x[1],False)\n",
    "#Final.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output top 10 pairs \n",
    "1. Collect the sorted RDD(Final) by calling take(10) to get to 10 similarity pairs \n",
    "2. And then use iteration to output the result as the format: (document1, document2 : similarity)\n",
    "3. 結果以一般小數點表示(四捨五入至小數點第三位)4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 20) : 1.0\n",
      "(52, 84) : 1.0\n",
      "(47, 49) : 0.756\n",
      "(30, 35) : 0.707\n",
      "(49, 88) : 0.509\n",
      "(48, 49) : 0.484\n",
      "(23, 38) : 0.482\n",
      "(14, 40) : 0.401\n",
      "(47, 88) : 0.385\n",
      "(47, 48) : 0.366\n"
     ]
    }
   ],
   "source": [
    "for i in Final.take(10):\n",
    "    pair = str(i[0])\n",
    "    valid_f = '{:.3}'.format(i[1])\n",
    "    sim = str(valid_f)\n",
    "    print(pair+\" : \"+sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
